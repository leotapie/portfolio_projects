{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5639636f",
   "metadata": {},
   "source": [
    "# Project: Predicting future NBA Most Valuable Player (MVP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda0d75",
   "metadata": {},
   "source": [
    "# Part I: Aquiring historical data - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bca93",
   "metadata": {},
   "source": [
    "In the first part of this project, we'll scrap the different pieces of data we'll need for our analysis and prediction model. \n",
    "\n",
    "All the data will be scrapped from the [**Basketball Reference**](https://www.basketball-reference.com/), which a reliable and extensive resource for basketball stats. It alsa has the benefit to be very well structured. For this project we'll use data from 1991 to 2023 (2023 being the last complete season to data).\n",
    "\n",
    "\n",
    "We're going to extra 3 sets of data:\n",
    "* MVP Award Ranking for each season (contains the point count from the vote and stats for each player nomminee) \n",
    "* All players stats for each season \n",
    "* Teams Stats for each season"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2a9f7",
   "metadata": {},
   "source": [
    "## Format of the data (1991 as an example):\n",
    "\n",
    "* [**MVP AWARD RANKING**](https://www.basketball-reference.com/awards/awards_1991.html)\n",
    "* [**PLAYERS STATS**](https://www.basketball-reference.com/leagues/NBA_1991_per_game.html)\n",
    "* [**TEAMS STATS**](https://www.basketball-reference.com/leagues/NBA_1991_standings.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94c3f4b-9099-4a31-a5b4-f1e47f5327bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries we need:\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba81e8c",
   "metadata": {},
   "source": [
    "## MVP Award Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5762d7",
   "metadata": {},
   "source": [
    "The **Basketball Reference** website is well made and the *MVP Award Ranking* for each year is formated in the same way. So we can simply iterate through the years we are interested in and download each .html page using the `requests` library. Saving the webpages locally allows to minimise the amount of request we submit to the site (best practice!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9e8769-d3bc-47ce-a42c-76384dd2d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of years (range is exclusive at the top-end):\n",
    "years = list(range(1991,2024))\n",
    "\n",
    "# Baseline url\n",
    "url_base_mvp = \"https://www.basketball-reference.com/awards/awards_{}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9b38a0-bb51-4b3b-8f19-dffde087eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through years and save each year's webpage to the mvp folder in our directory\n",
    "for year in years:\n",
    "    \n",
    "    # Use the *time* library to remain within request limits of website\n",
    "    time.sleep(1)\n",
    "    \n",
    "    url = url_base_mvp.format(year)\n",
    "    \n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(\"mvp/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd3915",
   "metadata": {},
   "source": [
    "Next, for each webpage we'll parse the table we are interested in. To do so we'll use `BeautifulSoup` which will allow us to initialise a parser class to extract the table. We'll then save all the data in one big pandas dataframe (as we iterate through each year, we'll actually create a list of dataframes but will combine these into our final datframe at the end).\n",
    "\n",
    "We know the structure of the table we want to extract, in this case there's a double header which will be impractical for pandas so we'll go ahead and remove the top header. Looking at the source code we've identified the header element tag: **'tr' (class_=\"over_header\")** - so will remove that before reading the table of the .html file.\n",
    "\n",
    "Finally, we know that the table has a specific id tag **'mvp'** that we'll use to extract the data we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909976ae-457f-430d-be07-f831b6374191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a72eb92-9803-4673-95d5-07aa96965380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise our list of dataframe: \n",
    "dfs = []\n",
    "\n",
    "# Iterate through each year:\n",
    "for year in years:\n",
    "    # First, read the content of the page (string format)\n",
    "    with open(\"mvp/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    # Then, we initalise BeautifulSoup as an .html parser\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Remove the top header of the table\n",
    "    soup.find('tr', class_=\"over_header\").decompose()\n",
    "    \n",
    "    # Use the table 'id' to extract the data\n",
    "    mvp_table = soup.find_all(id=\"mvp\")[0]\n",
    "    \n",
    "    # We can use pandas capability to read html (note: table need to be converted to string)\n",
    "    mvp_df = pd.read_html(str(mvp_table))[0]\n",
    "    \n",
    "    # Add the year of the data we've just extracted as we'll combine all years\n",
    "    mvp_df[\"Year\"] = year\n",
    "    \n",
    "    # Finally append to our pre-defined list\n",
    "    dfs.append(mvp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332956d",
   "metadata": {},
   "source": [
    "Let's combine the data from each year into one dataframe and save it to .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f359a29-1b04-4981-a097-26a6369ceb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps = pd.concat(dfs)\n",
    "#mvps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01c744f-abe2-47c7-89eb-5f25b4154baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps.to_csv(\"mvps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575377e",
   "metadata": {},
   "source": [
    "## Players stats\n",
    "\n",
    "Now that we have the scraped the data for the **MVP Award Ranking**, we'll do the same for the **Player Stats** - this will contain the stats from all players in the league, not just the MVP nominee. We'll be able to use this data to compare *MVP worthy stats/performance* to the rest of the league to train our ML algorithm in part 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a8db8",
   "metadata": {},
   "source": [
    "In this case, the table we're interested in isn't fully loaded when we open the page - therefore when we send a request it only return what is loaded initially. When we visit the page, the rest of the table is rendered as we scroll down (scrolling down actually triggers a java script to be run). Essentially, rather than just sending a request, we need to *run* that script within the browser (to generate all the data) before extracting it. \n",
    "\n",
    "To do so we can use `selenium` package and **driver**. We'll install the Chrome driver from [here](https://chromedriver.chromium.org/downloads), this will will allow python to automate the browser. We'll just need to store the executable file in sensible location. We can then control the browser using `selenium`to visit different pages or in our case fully load a page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cede6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline url\n",
    "url_base_player = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b7648f-7e40-4c97-b5ef-ba94518fef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba723c79-2d89-4ca9-aa88-9629fbca1802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xattr: /Users/Leo/chromedriver: No such xattr: com.apple.quarantine\r\n"
     ]
    }
   ],
   "source": [
    "# On mac we need to run this line as chrome driver hasn't been verified by Apple\n",
    "!xattr -d com.apple.quarantine /Users/Leo/chromedriver\n",
    "\n",
    "# Initialise the driver:\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/Leo/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4301cb-2350-46fd-bc8b-f1d23d2852b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab the html of each year\n",
    "for year in years:\n",
    "    \n",
    "    url = url_base_player.format(year)\n",
    "    \n",
    "    # Tell the driver to render the url in the browser \n",
    "    driver.get(url)\n",
    "    \n",
    "    # Execute the java script to make sure we render the whole table\n",
    "    driver.execute_script(\"window.scrollTo(1,10000)\")\n",
    "    \n",
    "    # Use the *time* library to allow for the java script to be executed\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Write and save the data to a file in our player folder\n",
    "    with open(\"player/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a505d1",
   "metadata": {},
   "source": [
    "Similarly to the **MVP Award Ranking**, we'll parse the table of interest from the html pages we've just saved. We'll also have to remove the extra headers and at a `year`column to each data frame. Finally we'll combine all the dfs into one big one and saved it to .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c82bfbf-cbba-4128-b04f-53147ba50f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"player/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    player_table = soup.find_all(id=\"per_game_stats\")[0]\n",
    "    player_df = pd.read_html(str(player_table))[0]\n",
    "    player_df[\"Year\"] = year\n",
    "    dfs.append(player_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08ff2b7-8e29-49bc-8a99-e9a0445d7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.concat(dfs)\n",
    "#players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04645c30-9811-44d1-97b7-7bd6edb3cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "players.to_csv(\"players.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509810f5",
   "metadata": {},
   "source": [
    "## Teams Stats\n",
    "\n",
    "Now on to **Team Stats**. Teams W/L rate is an important factor for the MVP voting, we'll therefore use it as a predictor in our ML model in part 2.\n",
    "\n",
    "We'll extract and then combine the data from the two *Standing* tables (Eastern and Western conferences). In this case we can simply use the `request`library without going through a driver. \n",
    "\n",
    "Note: *We could also get the data from the **Extended Standing** table, but this would required a browser driver and some manipulation in Pandas to seperate the W/L column of interest. We'll use the other method we've mentioned above to familiarise ouselves with scraping and combining multiple tables.*\n",
    "\n",
    "Finally we'll combine all the dfs into one big one and saved it to .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "485c5f1b-a66f-4707-ac4f-3ca236eb2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline url\n",
    "url_base_team = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74af1956-d588-4554-80a4-4e25771fcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    url = url_base_team.format(year)\n",
    "    \n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(\"team/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd117b-f847-4da2-9f37-171df6a94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"team/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    \n",
    "    team_table = soup.find_all(id=\"divs_standings_E\")[0]\n",
    "    team_df = pd.read_html(str(team_table))[0]\n",
    "    team_df[\"Year\"] = year\n",
    "    \n",
    "    # We'll add create a common team column for the data comming from each table, and delete the initial column\n",
    "    team_df[\"Team\"] = team_df[\"Eastern Conference\"]\n",
    "    del team_df[\"Eastern Conference\"]\n",
    "    \n",
    "    dfs.append(team_df)\n",
    "    \n",
    "    team_table = soup.find_all(id=\"divs_standings_W\")[0]\n",
    "    team_df = pd.read_html(str(team_table))[0]\n",
    "    team_df[\"Year\"] = year\n",
    "    \n",
    "    # Same as above\n",
    "    team_df[\"Team\"] = team_df[\"Western Conference\"]\n",
    "    del team_df[\"Western Conference\"]\n",
    "    \n",
    "    dfs.append(team_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193e14a-0c5e-4f18-820c-1e62140c927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.concat(dfs)\n",
    "#teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad744882-06c4-4e9c-a752-88677ec27668",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.to_csv(\"teams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4b95a",
   "metadata": {},
   "source": [
    "### This is the end of Part 1 - We now have the data we need saved in 3 different .csv, ready for the next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
